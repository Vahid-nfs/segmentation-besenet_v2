from model import Custom_model import  pandas as pdimport torchimport torch.nn as nnimport torch.nn.functional as F  from torch.utils.data import Dataset, DataLoader   from torchvision import transforms  import os import  cv2import  numpy as npfrom glob import  globimport segmentation_models_pytorch as smpfrom segmentation_models_pytorch import utilsfrom sklearn.model_selection import train_test_splitfrom torchsummary import summary#paramlast_epoch=0num_epochs=2CLASSES = 13checkpoint_path="./checkpoints"train_data=glob("./cityscapes_data/train/*.jpg")val_data=glob("./cityscapes_data/val/*.jpg")train_dataset=pd.DataFrame(train_data,columns=["img_path"])train_dataset=train_dataset.sample(frac=1)val_dataset=pd.DataFrame(val_data,columns=["img_path"])    os.makedirs(checkpoint_path ,exist_ok=True)    class costum_dataset(Dataset):    def __init__(self,dataframe,transform=None,target_transform=None):        super().__init__()        self.df=dataframe        self.trans=transform        self.tar_trans=target_transform            def __len__(self):        return len(self.df)        def __getitem__(self,index):        img=cv2.imread(self.df.iloc[index]["img_path"])        image,mask=img[:,:256,:],img[:,256:,:]                if self.trans:            image=self.trans(image)                    if self.tar_trans :            mask=self.tar_trans(mask)                            return(image.float(),mask.float())        def bin_image(mask):    bins = np.array([20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240])    new_mask = np.digitize(mask, bins)    return new_mask           def target_transform (mask):    mask=cv2.cvtColor(mask,cv2.COLOR_RGB2GRAY)    mask=mask=bin_image(mask)    mask=F.one_hot(torch.tensor(mask).long(),CLASSES)    mask=torch.permute(mask,(2,0,1))    return(mask)transform=transforms.Compose([transforms.ToTensor()])                      train_set=costum_dataset(train_dataset, transform=transform, target_transform=target_transform)val_set=costum_dataset(val_dataset, transform=transform, target_transform=target_transform)train_loader=DataLoader(train_set,batch_size=64)val_loader=DataLoader(val_set,batch_size=32)if last_epoch >= 1:    model=Custom_model(CLASSES)    model.load_state_dict(torch.load(f"{checkpoint_path}/model_std_{last_epoch}.pth"))    print ("model loaded !")else:     model=Custom_model(CLASSES)model.train(True)print(summary(model,(3,256,256)))IoU=utils.metrics.IoU(threshold=0.5)optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0001),])criteria_pre = nn.CrossEntropyLoss()criteria_aux = [nn.CrossEntropyLoss() for _ in range(4)]total_step=len(train_loader)for epoch in range (last_epoch+1,num_epochs+1):    for step,(images,masks) in enumerate (train_loader):        logits, *logits_aux = model(images)        loss_pre = criteria_pre(logits, masks)        loss_aux = [crit(lgt, masks) for crit, lgt in zip(criteria_aux, logits_aux)]        loss = loss_pre + 0.8*sum(loss_aux)        optimizer.zero_grad()        loss.backward()        optimizer.step()        # lr_scheduler.step()        iou_metrics=IoU(logits, masks)        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, IoU: {:.2f} % '                         .format(epoch, num_epochs, step+1, total_step, loss.item(), iou_metrics.item()))    torch.save(model,f"{checkpoint_path}/model_{epoch}.pt")